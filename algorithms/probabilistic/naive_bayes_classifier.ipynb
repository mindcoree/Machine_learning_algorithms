{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Алгоритм наивного байесовского классификатора\n",
    "\n",
    "В основе наивного байесовского классификатора лежит теорема Байеса. В данном случае, мы классифицируем объекты на два класса: $y = -1$ и $y = +1$. Признаки векторов $x = [x_1, x_2]^T$ считаются статистически независимыми и распределенными по нормальному закону.\n",
    "\n",
    "**Общая формула плотности вероятности нормального распределения (Гаусса)** для одной случайной величины $x$ с математическим ожиданием $\\mu$ и дисперсией $\\sigma^2$ выглядит следующим образом:\n",
    "\n",
    "$$f(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "Поскольку признаки $x_1$ и $x_2$ считаются статистически независимыми, условная вероятность $p(x | y)$ может быть представлена как произведение условных вероятностей каждого признака, где каждый признак $x_i$ распределен нормально:\n",
    "$$p(x | y) = p(x_1 | y) \\cdot p(x_2 | y)$$\n",
    "\n",
    "#### Условные вероятности признаков\n",
    "\n",
    "Применяя формулу нормального распределения к каждому признаку, получаем:\n",
    "\n",
    "**Для класса $y = -1$**: Условная вероятность $p(x | y = -1)$ рассчитывается как произведение вероятностей отдельных признаков, предполагая их независимость и нормальное распределение:\n",
    "\n",
    "$$p(x_i | y = -1) = \\frac{1}{2\\pi \\cdot \\sqrt{D_1^{(-1)} \\cdot D_2^{(-1)}}} \\cdot exp\\left(-\\frac{(x_1 - m_{x1}^{(-1)})^2}{2D_1^{(-1)}} - \\frac{(x_2 - m_{x2}^{(-1)})^2}{2D_2^{(-1)}}\\right)$$\n",
    "\n",
    "Где:\n",
    "- $m_{x1}^{(-1)}, m_{x2}^{(-1)}$ — математические ожидания для признаков $x_1, x_2$ класса $y = -1$.\n",
    "- $D_1^{(-1)}, D_2^{(-1)}$ — дисперсии признаков $x_1, x_2$ класса $y = -1$.\n",
    "\n",
    "**Для класса $y = +1$**: Аналогично, условная вероятность $p(x | y = +1)$, также основанная на предположении о нормальном распределении и независимости признаков:\n",
    "\n",
    "$$p(x_i | y = +1) = \\frac{1}{2\\pi \\cdot \\sqrt{D_1^{(+1)} \\cdot D_2^{(+1)}}} \\cdot exp\\left(-\\frac{(x_1 - m_{x1}^{(+1)})^2}{2D_1^{(+1)}} - \\frac{(x_2 - m_{x2}^{(+1)})^2}{2D_2^{(+1)}}\\right)$$\n",
    "\n",
    "Где:\n",
    "- $m_{x1}^{(+1)}, m_{x2}^{(+1)}$ — математические ожидания для признаков $x_1, x_2$ класса $y = +1$.\n",
    "- $D_1^{(+1)}, D_2^{(+1)}$ — дисперсии признаков $x_1, x_2$ класса $y = +1$.\n",
    "\n",
    "#### Классификация\n",
    "\n",
    "Цель классификации — найти класс $y$, который максимизирует апостериорную вероятность $P(y|x)$. Согласно теореме Байеса, эта вероятность выражается как:\n",
    "\n",
    "$$P(y|x) = \\frac{p(x|y)P(y)}{p(x)}$$\n",
    "\n",
    "При наличии штрафов за неверную классификацию ($\\lambda_y$), правило классификации модифицируется для максимизации взвешенной апостериорной вероятности. Таким образом, мы выбираем класс $y$, который максимизирует произведение $\\lambda_y \\cdot P(y|x)$. Подставляя формулу Байеса, получаем:\n",
    "\n",
    "$\\operatorname{argmax}_{y \\in Y} (\\lambda_y \\cdot \\frac{p(x|y)P(y)}{p(x)})$\n",
    "\n",
    "Поскольку $p(x)$ (вероятность наблюдения признаков $x$) является константой для всех классов $y$ и не влияет на выбор класса с максимальной вероятностью, мы можем исключить ее из максимизации. Таким образом, правило упрощается до:\n",
    "\n",
    "$$\\operatorname{argmax}_{y \\in Y} (\\lambda_y \\cdot p(x|y)P(y))$$\n",
    "\n",
    "Для удобства вычислений и предотвращения возникновения очень малых чисел (что может привести к ошибкам округления), обычно используется логарифм этого выражения, так как функция логарифма является монотонно возрастающей и не изменяет положение максимума:\n",
    "\n",
    "$$a(x) = \\operatorname{argmax}_{y \\in Y} (\\ln(\\lambda_y \\cdot P_y) + \\ln(p(x | y)))$$\n",
    "\n",
    "Где:\n",
    "- $Y$ — множество всех классов (в данном случае, $Y = \\{-1, +1\\}$).\n",
    "- $\\lambda_y$ — штраф за неверную классификацию класса $y$.\n",
    "- $P_y$ — априорная вероятность появления образов класса $y$.\n",
    "- $p(x | y)$ — условная вероятность признаков $x$ при условии принадлежности к классу $y$, рассчитанная по формулам выше.\n",
    "\n",
    "На практике, для каждого объекта $x$ вычисляются значения логарифмических функций для каждого класса, и объект относится к тому классу, для которого это значение максимально.\n",
    "\n",
    "#### Оценка качества классификатора\n",
    "\n",
    "Для оценки качества классификации используется показатель $Q$, который представляет собой долю неверно классифицированных объектов:\n",
    "\n",
    "$$Q(a,X) = \\frac{1}{n} \\sum_{i=1}^{n} [a(x_i) \\neq y_i]$$\n",
    "\n",
    "где:\n",
    "- $n$ — общее количество объектов в выборке.\n",
    "- $a(x_i)$ — предсказанный класс для объекта $x_i$.\n",
    "- $y_i$ — истинный класс объекта $x_i$.\n",
    "- $[\\cdot]$ — скобки Айверсона, означающие, что выражение равно 1, если условие внутри истинно, и 0 в противном случае."
   ],
   "id": "6c0e5ffe57e9a369"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-12T13:04:38.514723Z",
     "start_time": "2025-11-12T13:04:38.461793Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "data_x = [(7.2, 2.5), (6.4, 2.2), (6.3, 1.5), (7.7, 2.2), (6.2, 1.8), (5.7, 1.3), (7.1, 2.1), (5.8, 2.4), (5.2, 1.4), (5.9, 1.5), (7.0, 1.4), (6.8, 2.1), (7.2, 1.6), (6.7, 2.4), (6.0, 1.5), (5.1, 1.1), (6.6, 1.3), (6.1, 1.4), (6.7, 2.1), (6.4, 1.8), (5.6, 1.3), (6.9, 2.3), (6.4, 1.9), (6.9, 2.3), (6.5, 2.2), (6.0, 1.5), (5.6, 1.1), (5.6, 1.5), (6.0, 1.0), (6.0, 1.8), (6.7, 2.5), (7.7, 2.3), (5.5, 1.1), (5.8, 1.0), (6.9, 2.1), (6.6, 1.4), (6.3, 1.6), (6.1, 1.4), (5.0, 1.0), (7.7, 2.0), (4.9, 1.7), (7.2, 1.8), (6.8, 1.4), (6.1, 1.2), (5.8, 1.9), (6.3, 2.5), (5.7, 2.0), (6.5, 1.8), (7.6, 2.1), (6.3, 1.5), (6.7, 1.4), (6.4, 2.3), (6.2, 2.3), (6.3, 1.9), (5.5, 1.3), (7.9, 2.0), (6.7, 1.8), (6.4, 1.3), (6.5, 2.0), (6.5, 1.5), (6.9, 1.5), (5.6, 1.3), (5.8, 1.2), (6.7, 2.3), (6.0, 1.6), (5.7, 1.2), (5.7, 1.0), (5.5, 1.0), (6.1, 1.4), (6.3, 1.8), (5.7, 1.3), (6.1, 1.3), (5.5, 1.3), (6.3, 1.3), (5.9, 1.8), (7.7, 2.3), (6.5, 2.0), (5.6, 2.0), (6.7, 1.7), (5.7, 1.3), (5.5, 1.2), (5.0, 1.0), (5.8, 1.9), (6.2, 1.3), (6.2, 1.5), (6.3, 2.4), (6.4, 1.5), (7.4, 1.9), (6.8, 2.3), (5.6, 1.3), (5.8, 1.2), (7.3, 1.8), (6.7, 1.5), (6.3, 1.8), (6.0, 1.6), (6.4, 2.1), (6.1, 1.8), (5.9, 1.8), (5.4, 1.5), (4.9, 1.0)]\n",
    "data_y = [1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1]\n",
    "\n",
    "x_train = np.array(data_x)\n",
    "y_train = np.array(data_y)\n",
    "\n",
    "# математические ожидания\n",
    "mx11, mx12 = np.mean(x_train[y_train == -1], axis=0)\n",
    "mx21, mx22 = np.mean(x_train[y_train == 1], axis=0)\n",
    "\n",
    "# дисперсии\n",
    "Dx11, Dx12 = np.var(x_train[y_train == -1], axis=0)\n",
    "Dx21, Dx22 = np.var(x_train[y_train == 1], axis=0)\n",
    "\n",
    "lm1 = 1     # штраф неверной классификации 1-го класса (-1)\n",
    "lm2 = 1     # штраф неверной классификации 2-го класса (+1)\n",
    "P1 = 0.5    # априорная вероятность появления образов 1-го класса\n",
    "P2 = 1 - P1 # априорная вероятность появления образов 2-го класса\n",
    "\n",
    "# здесь продолжайте программу\n",
    "p_1 = lambda x1,x2: (1 / (2*np.pi * np.sqrt(Dx11 * Dx12))) * np.exp(-((x1 - mx11)**2/(2*Dx11)) - ((x2 - mx12)**2/(2*Dx12)))\n",
    "p_2 = lambda x1,x2: (1 / (2*np.pi * np.sqrt(Dx21 * Dx22))) * np.exp(-((x1 - mx21)**2/(2*Dx21)) - ((x2 - mx22)**2/(2*Dx22)))\n",
    "\n",
    "predict = []\n",
    "for x1,x2 in x_train:\n",
    "    a_1 = np.log(lm1*P1) + np.log(p_1(x1,x2))\n",
    "    a_2 = np.log(lm2*P2) + np.log(p_2(x1,x2))\n",
    "    predict.append(np.argmax([a_1,a_2])*2 - 1)\n",
    "pred = np.array(predict)\n",
    "Q = np.mean(pred != y_train)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Алгоритм регрессии с использованием теоремы Байеса (метод максимального правдоподобия)\n",
    "\n",
    "В задачах регрессии мы стремимся аппроксимировать неизвестную функцию $f(x)$ с помощью модели $a(x)$, параметры которой определяются на основе обучающих данных. В данном случае, функция имеет вид:\n",
    "\n",
    "$$f(x) = 0.5 \\cdot x + 0.2 \\cdot x^2 - 0.05 \\cdot x^3 + 0.2 \\cdot \\sin(4x) - 2.5$$\n",
    "\n",
    "и аппроксимируется линейной моделью вида:\n",
    "\n",
    "$$a(x) = w_0 + w_1 \\cdot x + w_2 \\cdot x^2 + w_3 \\cdot x^3$$\n",
    "\n",
    "Вектор параметров $w = [w_0, w_1, w_2, w_3]^T$ находится путем максимизации функции правдоподобия, что эквивалентно максимизации апостериорной вероятности $p(y|x,w)$ в рамках байесовского подхода:\n",
    "\n",
    "$$w^* = \\operatorname{argmax}_w p(y|x,w)$$\n",
    "\n",
    "#### Вероятностная модель\n",
    "\n",
    "Для простоты будем полагать, что отсчеты ${y_i}$ статистически независимы и подчиняются гауссовскому распределению (нормальной плотности распределения вероятностей - ПРВ). Плотность вероятности для одного наблюдения $y_i$ при заданных $x_i$ и $w$ имеет вид:\n",
    "\n",
    "$$p(y_i|x_i,w) = \\frac{1}{\\sqrt{2\\pi \\cdot D_e}} \\cdot \\exp\\left(-\\frac{1}{2D_e} \\cdot (y_i - w^T \\cdot x_i)^2\\right)$$\n",
    "\n",
    "где:\n",
    "- $x_i = [1, x_i, x_i^2, x_i^3]^T$ — вектор признаков $i$-го образа;\n",
    "- $y_i$ — значение функции $f(x)$ в $i$-й точке;\n",
    "- $D_e$ — дисперсия шума (предполагается постоянной для всех наблюдений).\n",
    "\n",
    "Поскольку наблюдения ${y_i}$ считаются статистически независимыми, многомерная ПРВ вектора выходных значений $y$ от входных значений $x$ и $w$ записывается как произведение индивидуальных ПРВ:\n",
    "\n",
    "$$p(y|x,w) = \\prod_{i=1}^n p(y_i|x_i,w)$$\n",
    "\n",
    "Подставляя выражение для $p(y_i|x_i,w)$, получаем:\n",
    "\n",
    "$$p(y|x,w) = \\frac{1}{(2\\pi \\cdot D_e)^{n/2}} \\cdot \\exp\\left(-\\frac{1}{2D_e} \\cdot \\sum_{i=1}^n (y_i - w^T \\cdot x_i)^2\\right)$$\n",
    "\n",
    "#### Нахождение оптимального вектора параметров $w$\n",
    "\n",
    "Для нахождения точки максимума $w^*$ ПРВ $p(y|x,w)$, удобно максимизировать её логарифм (поскольку логарифм — монотонно возрастающая функция, положение максимума не меняется). Сначала возьмём натуральный логарифм от $p(y|x,w)$:\n",
    "\n",
    "$$\\ln p(y|x,w) = \\ln\\left( \\frac{1}{(2\\pi \\cdot D_e)^{n/2}} \\right) - \\frac{1}{2D_e} \\cdot \\sum_{i=1}^n (y_i - w^T \\cdot x_i)^2$$\n",
    "\n",
    "$$\\ln p(y|x,w) = -\\frac{n}{2}\\ln(2\\pi D_e) - \\frac{1}{2D_e} \\cdot \\sum_{i=1}^n (y_i - w^T \\cdot x_i)^2$$\n",
    "\n",
    "Первый член ($-n/2 \\ln(2\\pi D_e)$) не зависит от $w$, поэтому для максимизации $\\ln p(y|x,w)$ нам нужно минимизировать второй член, который представляет собой сумму квадратов ошибок. Это эквивалентно минимизации выражения:\n",
    "\n",
    "$$\\sum_{i=1}^n (y_i - w^T \\cdot x_i)^2$$\n",
    "\n",
    "что является **методом наименьших квадратов**. Чтобы найти $w^*$, мы берём производную логарифма по $w$ и приравниваем её к нулю:\n",
    "\n",
    "$$\\frac{\\partial \\ln p(y|x,w)}{\\partial w} = \\frac{1}{D_e} \\cdot \\sum_{i=1}^n (y_i - w^T \\cdot x_i) \\cdot x_i^T = 0$$\n",
    "\n",
    "Это уравнение приводит к хорошо известному решению для метода наименьших квадратов:\n",
    "\n",
    "$$w^* = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "где $X$ — матрица признаков, а $y$ — вектор целевых значений.\n",
    "\n",
    "#### Оценка качества модели\n",
    "\n",
    "Для оценки качества регрессионной модели обычно используется средний эмпирический риск (Mean Squared Error - MSE):\n",
    "\n",
    "$$Q = \\frac{1}{n} \\sum_{i=1}^n (y_i - a(x_i))^2$$"
   ],
   "id": "6f06100a888ae85e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T13:05:10.642605Z",
     "start_time": "2025-11-12T13:05:10.634676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#3.4.5\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def func(x):\n",
    "    return 0.5 * x + 0.2 * x ** 2 - 0.05 * x ** 3 + 0.2 * np.sin(4 * x) - 2.5\n",
    "\n",
    "\n",
    "def model(w, x):\n",
    "    return w[0] + w[1] * x + w[2] * x ** 2 + w[3] * x ** 3\n",
    "\n",
    "\n",
    "coord_x = np.arange(-4.0, 6.0, 0.1)\n",
    "\n",
    "x_train = np.array([[_x**i for i in range(4)] for _x in coord_x]) # обучающая выборка\n",
    "y_train = func(coord_x) # целевые выходные значения\n",
    "\n",
    "# здесь продолжайте программу\n",
    "w = np.linalg.inv(x_train.T @ x_train) @ x_train.T @ y_train\n",
    "\n",
    "# Средний эмпирический риск\n",
    "Q = np.mean((y_train - x_train @ w)**2)\n",
    "\n",
    "w, Q\n"
   ],
   "id": "f8336f48dec5c034",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.49213311,  0.50342222,  0.19781693, -0.04986103]),\n",
       " np.float64(0.0196629429495297))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
