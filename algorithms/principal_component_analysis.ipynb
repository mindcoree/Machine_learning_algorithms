{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd66850",
   "metadata": {},
   "source": [
    "# **Principal Component Analysis (PCA)**\n",
    "\n",
    "Principal Component Analysis (PCA) — это метод, который позволяет **сократить размерность данных**, оставив только наиболее информативные признаки. Основная идея PCA — найти такие направления в пространстве признаков, вдоль которых данные имеют **максимальную дисперсию**, и преобразовать данные в новое пространство, где признаки **некоррелированы**.\n",
    "\n",
    "### 1. Собственные числа и собственные векторы\n",
    "\n",
    "Для PCA мы используем либо **ковариационную матрицу**:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{n-1} (X - \\bar X)^\\top (X - \\bar X)\n",
    "$$\n",
    "\n",
    "либо **матрицу Грама** для разреженных или нецентрированных данных:\n",
    "\n",
    "$$\n",
    "G = \\frac{1}{n} X^\\top X\n",
    "$$\n",
    "\n",
    "Далее решаем уравнение на собственные значения и собственные векторы:\n",
    "\n",
    "$$\n",
    "A w = \\lambda w\n",
    "$$\n",
    "\n",
    "где\n",
    "\n",
    "* $A$ — ковариационная матрица или матрица Грама,\n",
    "* $w$ — собственный вектор (направление главной компоненты),\n",
    "* $\\lambda$ — собственное число (дисперсия вдоль этого направления).\n",
    "\n",
    "При действии матрицы $A$ на собственный вектор $w$ **его направление не меняется**, меняется только масштаб на величину $\\lambda$.\n",
    "\n",
    "Собственные значения определяются из уравнения:\n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "а для каждого $\\lambda$ находим соответствующий собственный вектор $w$.\n",
    "\n",
    "\n",
    "### 2. Матрица собственных векторов\n",
    "\n",
    "Собственные векторы собираются в матрицу $W$, где **каждый столбец — собственный вектор**:\n",
    "\n",
    "$$\n",
    "W = [w_1, w_2, \\dots, w_m]\n",
    "$$\n",
    "\n",
    "Эта матрица **ортонормирована**, то есть $$W^\\top W = I$$. Это означает, что столбцы $W$ перпендикулярны друг другу и имеют единичную длину.\n",
    "\n",
    "\n",
    "### 3. Преобразование данных\n",
    "\n",
    "Чтобы получить новые признаки, проецируем исходные данные $X$ на пространство главных компонент:\n",
    "\n",
    "$$\n",
    "Z = X W \\quad \\text{или} \\quad Z = (X - \\bar X) W\n",
    "$$\n",
    "\n",
    "* Новые признаки $Z$ некоррелированы.\n",
    "* Они упорядочены по **убыванию дисперсии**: первая компонента несёт больше всего информации, последняя — меньше.\n",
    "\n",
    "\n",
    "### 4. Отбор информативных компонент\n",
    "\n",
    "* Собственные числа $\\lambda_i$ показывают, насколько важна каждая компонента.\n",
    "* Компоненты с малой дисперсией (например, $\\lambda_i < 0.01$) считаются малозначимыми и могут быть **отброшены**.\n",
    "* Часто используют порог по **доле объяснённой дисперсии**, оставляя компоненты, покрывающие 90–95% суммарной дисперсии.\n",
    "\n",
    "\n",
    "### 5. Краткий алгоритм PCA\n",
    "\n",
    "1. Вычислить матрицу $A = X^\\top X / n$ или ковариацию.\n",
    "2. Найти собственные значения $\\lambda_i$ и собственные векторы $w_i$.\n",
    "3. Отсортировать собственные числа по убыванию и переставить столбцы $W$ соответствующим образом.\n",
    "4. Проецировать данные: $Z = X W$.\n",
    "5. Удалить малозначимые компоненты по $\\lambda_i$ или доле объяснённой дисперсии.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
